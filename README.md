Data Extraction: Extract historical and real-time data from financial APIs such as
Alpha Vantage and Yahoo Finance. This will enable us to have a comprehensive
dataset for analysis.
Data Transformation: Use Apache Spark to perform data transformations and
calculations. This step ensures that the data is in a usable format for analysis and
reporting.
Data Loading: Load the processed data into a relational database such as
PostgreSQL. This centralized data repository will support various analytical and
reporting needs.
Pipeline Automation: Automate the pipeline to run at regular intervals using
Apache Airflow. This ensures continuous data updates and reduces manual
intervention.
